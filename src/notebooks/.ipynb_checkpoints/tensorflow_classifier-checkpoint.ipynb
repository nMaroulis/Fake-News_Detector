{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys, io, os, errno, fileinput, csv\n",
    "import collections as cl\n",
    "from os import path\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib as plt\n",
    "import seaborn as sb\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as kr\n",
    "import itertools\n",
    "import warnings\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from joblib import dump, load\n",
    "\n",
    "\n",
    "trainset_file = \"../../Datasets/train_1.csv\"\n",
    "train_df = pd.read_csv(trainset_file,  sep=',')\n",
    "count_vect = CountVectorizer(max_features=2048, stop_words='english', lowercase=True, ngram_range=(1, 2))\n",
    "tfidf_transformer = TfidfTransformer(smooth_idf=False)\n",
    "# svd = TruncatedSVD(n_components=16384)\n",
    "\n",
    "train_df.head()\n",
    "\n",
    "sample_size = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "__Count - Tfidf Version Testing__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n"
     ]
    }
   ],
   "source": [
    "train_df = train_df.dropna(subset=['text'])\n",
    "X = count_vect.fit_transform(train_df[0:sample_size]['text'].astype('U'))\n",
    "X = tfidf_transformer.fit_transform(X)\n",
    "Y = train_df[0:sample_size]['label']\n",
    "Y = Y.values\n",
    "print(np.size(X,1))\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "cvscores = []\n",
    "\n",
    "layer1 = [ 8 , 16 , 32 , 64 , 128 , 128 , 64 , 32 , 16 , 8 ]\n",
    "layer2 = [ 16 , 16 , 32 , 16 , 16 , 8 , 16 , 32 , 64 , 128 ]\n",
    "\n",
    "kmodel = kr.models.Sequential()\n",
    "kmodel.add(kr.layers.Dense(512, input_dim=np.size(X,1), activation='relu'))\n",
    "kmodel.add(kr.layers.Dense(256, activation='relu'))\n",
    "kmodel.add(kr.layers.Dense(256, activation='relu'))\n",
    "kmodel.add(kr.layers.Dense(1, activation='sigmoid'))\n",
    "kmodel.summary()\n",
    "#adam_opt = kr.optimizers.Adam(lr=0.0003, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "kmodel.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "# kmodel.fit(X,Y, epochs=1000, batch_size=10, verbose=1, shuffle=True, class_weight=None, sample_weight=None) # train\n",
    "\n",
    "i = 0\n",
    "for train, test in kfold.split(X, Y):\n",
    "    \n",
    "    kmodel.fit(X[train],Y[train], epochs=5, batch_size=10, verbose=0, shuffle=True, class_weight=None, sample_weight=None) # train\n",
    "\n",
    "    i += 1\n",
    "    scores = kmodel.evaluate(X[test], Y[test], verbose=0)\n",
    "    \n",
    "    print(\"<%d> %s: %.2f%%\" %  (i,kmodel.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "\n",
    "    \n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "__FINAL MODEL with TfIdfVectorizer TEST__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8192\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 512)               4194816   \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 4,392,193\n",
      "Trainable params: 4,392,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "<1> acc: 89.00%\n",
      "<2> acc: 100.00%\n",
      "<3> acc: 100.00%\n",
      "<4> acc: 100.00%\n",
      "<5> acc: 100.00%\n",
      "<6> acc: 100.00%\n",
      "<7> acc: 100.00%\n",
      "<8> acc: 100.00%\n",
      "<9> acc: 100.00%\n",
      "<10> acc: 100.00%\n",
      "98.90% (+/- 3.30%)\n"
     ]
    }
   ],
   "source": [
    "train_df = train_df.dropna(subset=['text'])\n",
    "vectorizer.fit(train_df[0:sample_size]['text'].astype('U'))\n",
    "X = vectorizer.transform(train_df[0:sample_size]['text'].astype('U'))\n",
    "Y = train_df[0:sample_size]['label']\n",
    "Y = Y.values\n",
    "print(np.size(X,1))\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "cvscores = []\n",
    "\n",
    "layer1 = [ 8 , 16 , 32 , 64 , 128 , 128 , 64 , 32 , 16 , 8 ]\n",
    "layer2 = [ 16 , 16 , 32 , 16 , 16 , 8 , 16 , 32 , 64 , 128 ]\n",
    "\n",
    "kmodel = kr.models.Sequential()\n",
    "kmodel.add(kr.layers.Dense(512, input_dim=np.size(X,1), activation='relu'))\n",
    "kmodel.add(kr.layers.Dense(256, activation='relu'))\n",
    "kmodel.add(kr.layers.Dense(256, activation='relu'))\n",
    "kmodel.add(kr.layers.Dense(1, activation='sigmoid'))\n",
    "kmodel.summary()\n",
    "#adam_opt = kr.optimizers.Adam(lr=0.0003, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "kmodel.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "# kmodel.fit(X,Y, epochs=1000, batch_size=10, verbose=1, shuffle=True, class_weight=None, sample_weight=None) # train\n",
    "\n",
    "i = 0\n",
    "for train, test in kfold.split(X, Y):\n",
    "    \n",
    "    kmodel.fit(X[train],Y[train], epochs=5, batch_size=64, verbose=0, shuffle=True, class_weight=None, sample_weight=None) # train\n",
    "\n",
    "    i += 1\n",
    "    scores = kmodel.evaluate(X[test], Y[test], verbose=0)\n",
    "    \n",
    "    print(\"<%d> %s: %.2f%%\" %  (i,kmodel.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "\n",
    "    \n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "__TEST Single Query__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    Ever get the feeling your life circles the rou...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = []\n",
    "test.append(\"Ever get the feeling your life circles the roundabout rather than heads in a straight line toward the intended destination? [Hillary Clinton remains the big woman on campus in leafy, liberal Wellesley, Massachusetts. Everywhere else votes her most likely to don her inauguration dress for the remainder of her days the way Miss Havisham forever wore that wedding dress.  Speaking of Great Expectations, Hillary Rodham overflowed with them 48 years ago when she first addressed a Wellesley graduating class. The president of the college informed those gathered in 1969 that the students needed â€œno debate so far as I could ascertain as to who their spokesman was\")\n",
    "\n",
    "test_x = vectorizer.transform(test)\n",
    "test_x\n",
    "prediction = kmodel.predict_classes(test_x)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "__FINAL TRAIN / SAVE__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8192\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 512)               4194816   \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 4,392,193\n",
      "Trainable params: 4,392,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "20761/20761 [==============================] - 19s 902us/sample - loss: 0.1596 - acc: 0.9347\n",
      "Epoch 2/60\n",
      "20761/20761 [==============================] - 18s 870us/sample - loss: 0.0291 - acc: 0.9905\n",
      "Epoch 3/60\n",
      "20761/20761 [==============================] - 20s 966us/sample - loss: 0.0093 - acc: 0.9971\n",
      "Epoch 4/60\n",
      "20761/20761 [==============================] - 30s 1ms/sample - loss: 0.0098 - acc: 0.9964\n",
      "Epoch 5/60\n",
      "20761/20761 [==============================] - 30s 1ms/sample - loss: 0.0066 - acc: 0.9978\n",
      "Epoch 6/60\n",
      "20761/20761 [==============================] - 32s 2ms/sample - loss: 0.0032 - acc: 0.9989\n",
      "Epoch 7/60\n",
      "20761/20761 [==============================] - 27s 1ms/sample - loss: 5.1827e-04 - acc: 1.0000\n",
      "Epoch 8/60\n",
      "20761/20761 [==============================] - 26s 1ms/sample - loss: 5.2302e-04 - acc: 0.9999\n",
      "Epoch 9/60\n",
      "20761/20761 [==============================] - 27s 1ms/sample - loss: 0.0012 - acc: 0.9997\n",
      "Epoch 10/60\n",
      "20761/20761 [==============================] - 27s 1ms/sample - loss: 0.0037 - acc: 0.9990\n",
      "Epoch 11/60\n",
      "20761/20761 [==============================] - 27s 1ms/sample - loss: 0.0056 - acc: 0.9982\n",
      "Epoch 12/60\n",
      "20761/20761 [==============================] - 26s 1ms/sample - loss: 0.0043 - acc: 0.9987\n",
      "Epoch 13/60\n",
      "20761/20761 [==============================] - 27s 1ms/sample - loss: 0.0020 - acc: 0.9995\n",
      "Epoch 14/60\n",
      "20761/20761 [==============================] - 27s 1ms/sample - loss: 0.0030 - acc: 0.9993\n",
      "Epoch 15/60\n",
      "20761/20761 [==============================] - 27s 1ms/sample - loss: 0.0012 - acc: 0.9997\n",
      "Epoch 16/60\n",
      "20761/20761 [==============================] - 27s 1ms/sample - loss: 4.5356e-04 - acc: 0.9999\n",
      "Epoch 17/60\n",
      "20761/20761 [==============================] - 27s 1ms/sample - loss: 0.0022 - acc: 0.9995\n",
      "Epoch 18/60\n",
      "20761/20761 [==============================] - 27s 1ms/sample - loss: 4.0257e-04 - acc: 1.0000\n",
      "Epoch 19/60\n",
      "20761/20761 [==============================] - 27s 1ms/sample - loss: 3.2577e-04 - acc: 1.0000\n",
      "Epoch 20/60\n",
      "20761/20761 [==============================] - 27s 1ms/sample - loss: 3.9289e-04 - acc: 1.0000\n",
      "Epoch 21/60\n",
      "20761/20761 [==============================] - 27s 1ms/sample - loss: 3.6375e-04 - acc: 1.0000\n",
      "Epoch 22/60\n",
      "20761/20761 [==============================] - 26s 1ms/sample - loss: 3.4816e-04 - acc: 1.0000\n",
      "Epoch 23/60\n",
      "20761/20761 [==============================] - 26s 1ms/sample - loss: 3.4149e-04 - acc: 1.0000\n",
      "Epoch 24/60\n",
      "20761/20761 [==============================] - 26s 1ms/sample - loss: 3.6513e-04 - acc: 1.0000\n",
      "Epoch 25/60\n",
      "20761/20761 [==============================] - 26s 1ms/sample - loss: 3.2655e-04 - acc: 1.0000\n",
      "Epoch 26/60\n",
      "20761/20761 [==============================] - 26s 1ms/sample - loss: 3.6311e-04 - acc: 1.0000\n",
      "Epoch 27/60\n",
      "20761/20761 [==============================] - 26s 1ms/sample - loss: 3.4012e-04 - acc: 1.0000\n",
      "Epoch 28/60\n",
      "20761/20761 [==============================] - 26s 1ms/sample - loss: 2.3418e-04 - acc: 1.0000\n",
      "Epoch 29/60\n",
      "20761/20761 [==============================] - 26s 1ms/sample - loss: 4.2163e-04 - acc: 1.0000\n",
      "Epoch 30/60\n",
      "20761/20761 [==============================] - 26s 1ms/sample - loss: 3.8026e-04 - acc: 1.0000\n",
      "Epoch 31/60\n",
      "20761/20761 [==============================] - 26s 1ms/sample - loss: 3.5725e-04 - acc: 1.0000\n",
      "Epoch 32/60\n",
      "20761/20761 [==============================] - 26s 1ms/sample - loss: 3.5902e-04 - acc: 1.0000\n",
      "Epoch 33/60\n",
      "20761/20761 [==============================] - 26s 1ms/sample - loss: 3.3685e-04 - acc: 1.0000\n",
      "Epoch 34/60\n",
      "20761/20761 [==============================] - 26s 1ms/sample - loss: 3.2265e-04 - acc: 1.0000\n",
      "Epoch 35/60\n",
      "20761/20761 [==============================] - 26s 1ms/sample - loss: 3.2373e-04 - acc: 1.0000\n",
      "Epoch 36/60\n",
      "20761/20761 [==============================] - 26s 1ms/sample - loss: 4.0420e-04 - acc: 1.0000\n",
      "Epoch 37/60\n",
      "20761/20761 [==============================] - 26s 1ms/sample - loss: 3.6179e-04 - acc: 1.0000\n",
      "Epoch 38/60\n",
      "20761/20761 [==============================] - 28s 1ms/sample - loss: 3.6098e-04 - acc: 1.0000\n",
      "Epoch 39/60\n",
      "20761/20761 [==============================] - 26s 1ms/sample - loss: 3.2157e-04 - acc: 1.0000\n",
      "Epoch 40/60\n",
      "20761/20761 [==============================] - 28s 1ms/sample - loss: 3.4504e-04 - acc: 1.0000\n",
      "Epoch 41/60\n",
      "20761/20761 [==============================] - 26s 1ms/sample - loss: 3.5089e-04 - acc: 1.0000\n",
      "Epoch 42/60\n",
      "20761/20761 [==============================] - 24s 1ms/sample - loss: 4.2719e-04 - acc: 1.0000\n",
      "Epoch 43/60\n",
      "20761/20761 [==============================] - 22s 1ms/sample - loss: 2.6919e-04 - acc: 1.0000\n",
      "Epoch 44/60\n",
      "20761/20761 [==============================] - 24s 1ms/sample - loss: 4.2870e-04 - acc: 1.0000\n",
      "Epoch 45/60\n",
      "20761/20761 [==============================] - 26s 1ms/sample - loss: 3.2252e-04 - acc: 1.0000\n",
      "Epoch 46/60\n",
      "20761/20761 [==============================] - 27s 1ms/sample - loss: 4.4096e-04 - acc: 1.0000\n",
      "Epoch 47/60\n",
      "20761/20761 [==============================] - 26s 1ms/sample - loss: 3.9803e-04 - acc: 1.0000\n",
      "Epoch 48/60\n",
      "20761/20761 [==============================] - 26s 1ms/sample - loss: 4.1374e-04 - acc: 1.0000\n",
      "Epoch 49/60\n",
      "20761/20761 [==============================] - 26s 1ms/sample - loss: 3.1982e-04 - acc: 1.0000\n",
      "Epoch 50/60\n",
      "20761/20761 [==============================] - 27s 1ms/sample - loss: 5.0369e-04 - acc: 1.0000\n",
      "Epoch 51/60\n",
      "20761/20761 [==============================] - 22s 1ms/sample - loss: 4.5916e-04 - acc: 1.0000\n",
      "Epoch 52/60\n",
      "20761/20761 [==============================] - 23s 1ms/sample - loss: 4.8520e-04 - acc: 1.0000\n",
      "Epoch 53/60\n",
      "20761/20761 [==============================] - 26s 1ms/sample - loss: 4.3345e-04 - acc: 1.0000\n",
      "Epoch 54/60\n",
      "20761/20761 [==============================] - 28s 1ms/sample - loss: 0.0240 - acc: 0.9960\n",
      "Epoch 55/60\n",
      "20761/20761 [==============================] - 26s 1ms/sample - loss: 0.0283 - acc: 0.9908\n",
      "Epoch 56/60\n",
      "20761/20761 [==============================] - 27s 1ms/sample - loss: 0.0032 - acc: 0.9991\n",
      "Epoch 57/60\n",
      "20761/20761 [==============================] - 26s 1ms/sample - loss: 3.4515e-04 - acc: 1.0000\n",
      "Epoch 58/60\n",
      "20761/20761 [==============================] - 26s 1ms/sample - loss: 3.1520e-04 - acc: 1.0000\n",
      "Epoch 59/60\n",
      "20761/20761 [==============================] - 26s 1ms/sample - loss: 3.0317e-04 - acc: 1.0000\n",
      "Epoch 60/60\n",
      "20761/20761 [==============================] - 26s 1ms/sample - loss: 2.9493e-04 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff71502f1d0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=8192, stop_words='english', lowercase=True, ngram_range=(1, 3),smooth_idf=False)\n",
    "\n",
    "\n",
    "train_df = train_df.dropna(subset=['text'])\n",
    "vectorizer.fit(train_df['text'].astype('U'))\n",
    "X = vectorizer.transform(train_df['text'].astype('U'))\n",
    "Y = train_df['label']\n",
    "Y = Y.values\n",
    "print(np.size(X,1))\n",
    "\n",
    "\n",
    "kmodel = kr.models.Sequential()\n",
    "kmodel.add(kr.layers.Dense(512, input_dim=np.size(X,1), activation='relu'))\n",
    "kmodel.add(kr.layers.Dense(256, activation='relu'))\n",
    "kmodel.add(kr.layers.Dense(256, activation='relu'))\n",
    "kmodel.add(kr.layers.Dense(1, activation='sigmoid'))\n",
    "kmodel.summary()\n",
    "kmodel.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "kmodel.fit(X,Y, epochs=60, batch_size=64, verbose=1, shuffle=True, class_weight=None, sample_weight=None) # train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "kmodel.save('../../models/keras_content_classifier.h5') # model = load_model('my_model.h5')\n",
    "#pickle.dump(vectorizer, open(\"../../models/tfidf.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "with open('../../models/tfidf.pickle', 'wb') as fin:\n",
    "    pickle.dump(vectorizer, fin)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
